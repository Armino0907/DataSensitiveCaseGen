{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.integrate import cumtrapz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractObjects(ocel):\n",
    "    df = pd.DataFrame(ocel.objects)\n",
    "    grouped = df.groupby('ocel:type')\n",
    "\n",
    "    # Create a dictionary to store DataFrames for each group\n",
    "    type_dataframes = {}\n",
    "\n",
    "    # Iterate through each group and store it in the dictionary\n",
    "    for group_name, group_data in grouped:\n",
    "        type_dataframes[group_name] = group_data.copy()\n",
    "\n",
    "    # Drop empty columns\n",
    "    for type_name, type_dataframe in type_dataframes.items():\n",
    "        type_dataframes[type_name].dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    return type_dataframes\n",
    "\n",
    "def extractObjectsDF(df):\n",
    "    grouped = df.groupby('ocel:type')\n",
    "\n",
    "    # Create a dictionary to store DataFrames for each group\n",
    "    type_dataframes = {}\n",
    "\n",
    "    # Iterate through each group and store it in the dictionary\n",
    "    for group_name, group_data in grouped:\n",
    "        type_dataframes[group_name] = group_data.copy()\n",
    "\n",
    "    # Drop empty columns\n",
    "    for type_name, type_dataframe in type_dataframes.items():\n",
    "        type_dataframes[type_name].dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    return type_dataframes\n",
    "\n",
    "#returns a list of all object types when given the data frame\n",
    "\n",
    "def extractObjectTypes(objects):\n",
    "    objectTypes = list(objects.keys())\n",
    "    return objectTypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustLogOLD(log):\n",
    "    o2os=log.o2o\n",
    "    objects=log.objects\n",
    "    # Filter objects dataframe to get items and products\n",
    "    items = objects[objects[\"ocel:type\"] == \"items\"]\n",
    "    products = objects[objects[\"ocel:type\"] == \"products\"]\n",
    "\n",
    "    # Filter o2os to get only the relationships where qualifier is \"is a\"\n",
    "    is_a_relationships = o2os[o2os[\"ocel:qualifier\"] == \"is a\"]\n",
    "\n",
    "    # Merge is_a_relationships with items to get items associated with products\n",
    "    items_with_relationships = items.merge(is_a_relationships, left_on=\"ocel:oid\", right_on=\"ocel:oid\")\n",
    "\n",
    "    # Merge with products to get the product attributes\n",
    "    items_with_product = items_with_relationships.merge(products, left_on=\"ocel:oid_2\", right_on=\"ocel:oid\", suffixes=(\"_item\", \"_product\"))\n",
    "\n",
    "    # Add the new 'product' column with the product id\n",
    "    items_with_product['product'] = items_with_product['ocel:oid_product']\n",
    "\n",
    "    # Select the relevant columns (including the new 'product' column)\n",
    "    items_with_product = items_with_product[[\"ocel:oid_item\", \"role_item\", \"weight_item\", \"price_item\", \"ocel:type_item\", \"product\"]]\n",
    "    items_with_product.columns = [\"ocel:oid\", \"role\", \"weight\", \"price\", \"ocel:type\", \"product\"]\n",
    "\n",
    "    # Remove products from the original objects dataframe\n",
    "    objects_no_products = objects[objects[\"ocel:type\"] != \"products\"]\n",
    "\n",
    "    # Update the objects dataframe with the new items_with_product dataframe\n",
    "    objects_updated = objects_no_products.merge(items_with_product, on=[\"ocel:oid\", \"role\", \"weight\", \"price\", \"ocel:type\"], how=\"left\")\n",
    "    \n",
    "    product_oids = objects[objects[\"ocel:type\"] == \"products\"][\"ocel:oid\"].tolist()\n",
    "\n",
    "    # Filter the 'o2os' dataframe to exclude rows where 'ocel:oid' or 'ocel:oid_2' is in 'product_oids'\n",
    "    o2os_filtered = o2os[~(o2os[\"ocel:oid\"].isin(product_oids) | o2os[\"ocel:oid_2\"].isin(product_oids))]\n",
    "\n",
    "    # Ensure product column is placed to the right\n",
    "    columns = objects_no_products.columns.tolist() + ['product']\n",
    "    objects_updated = objects_updated[columns]\n",
    "    # Custom function to determine if an item is premium\n",
    "    df = objects_updated\n",
    "    def is_premium(row):\n",
    "        if pd.notna(row['price']):\n",
    "            return row['price'] > 500.0\n",
    "        return None\n",
    "\n",
    "    # Add a new column 'is_premium' using the custom function\n",
    "    df['is_premium'] = df.apply(is_premium, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define a function to estimate volume based on weight\n",
    "    def calculate_volume(row):\n",
    "        if row['ocel:type'] == 'packages':\n",
    "            weight = row['weight']\n",
    "            # Define thresholds for volume based on weight\n",
    "            if weight < 1:\n",
    "                # Small package: volume is weight * 4000 (more compact)\n",
    "                return weight * 4000\n",
    "            elif 1 <= weight <= 5:\n",
    "                # Medium package: volume is weight * 5000\n",
    "                return weight * 5000\n",
    "            else:\n",
    "                # Large package: volume is weight * 6000 (bulkier packaging)\n",
    "                return weight * 6000\n",
    "        else:\n",
    "            # Return NaN for non-package types\n",
    "            return np.nan\n",
    "\n",
    "    # Apply the function to each row to create the 'volume' column\n",
    "    df['volume'] = df.apply(calculate_volume, axis=1)\n",
    "        \n",
    "    objects_updated = df\n",
    "        \n",
    "    return objects_updated,o2os_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustLog(log):\n",
    "    objects=log.objects\n",
    "    o2os = log.o2o\n",
    "    df = objects\n",
    "    def is_premium(row):\n",
    "        if pd.notna(row['price']):\n",
    "            return row['price'] > 500.0\n",
    "        return None\n",
    "\n",
    "    # Add a new column 'is_premium' using the custom function\n",
    "    df['is_premium'] = df.apply(is_premium, axis=1)\n",
    "    \n",
    "    objects = df\n",
    "    \n",
    "    # Step 1: Filter `objects` to get only customers and their `kdtyp`\n",
    "    # Rename `ocel:oid` as `customer_oid` for clarity\n",
    "    customers = objects[objects['ocel:type'] == 'customers'][['ocel:oid', 'kdtyp']].copy()\n",
    "    customers = customers.rename(columns={'ocel:oid': 'customer_oid'})\n",
    "\n",
    "    # Step 2: Filter `o2os` for \"places\" relationships\n",
    "    # Here, `ocel:oid` is the customer, and `ocel:oid_2` is the order\n",
    "    order_customer_relationships = o2os[o2os['ocel:qualifier'] == 'places'][['ocel:oid', 'ocel:oid_2']].copy()\n",
    "    order_customer_relationships = order_customer_relationships.rename(columns={'ocel:oid': 'customer_oid', 'ocel:oid_2': 'order_oid'})\n",
    "\n",
    "\n",
    "    # Step 3: Merge `order_customer_relationships` with `customers` to bring `kdtyp` into each order\n",
    "    order_with_kdtyp = order_customer_relationships.merge(customers, on='customer_oid', how='left')\n",
    "\n",
    "\n",
    "    # Step 4: Merge `order_with_kdtyp` back with the original `objects` DataFrame to add `kdtyp` to orders\n",
    "    # This will add `kdtyp` to each order in the original `objects` DataFrame based on the linked customer\n",
    "    final_objects = objects.merge(order_with_kdtyp[['order_oid', 'kdtyp']], left_on='ocel:oid', right_on='order_oid', how='left')\n",
    "\n",
    "    # Drop the extra `order_oid` column since `ocel:oid` already represents the order ID\n",
    "    final_objects = final_objects.drop(columns=['order_oid'])\n",
    "\n",
    "    # Merge the two `kdtyp` columns: prioritize the `kdtyp` from the orders, and use the original `kdtyp` where it exists for customers\n",
    "    final_objects['kdtyp'] = final_objects['kdtyp_x'].fillna(final_objects['kdtyp_y'])\n",
    "\n",
    "    # Drop the now-redundant `kdtyp_x` and `kdtyp_y` columns\n",
    "    final_objects = final_objects.drop(columns=['kdtyp_x', 'kdtyp_y'])\n",
    "    \n",
    "    final_objects = final_objects[final_objects[\"ocel:type\"].isin([\"items\",\"orders\",\"packages\"])]\n",
    "    \n",
    "    \n",
    "    return final_objects,o2os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(pd, title):\n",
    "    # Print the resulting probability distribution\n",
    "    probability_distribution = pd\n",
    "    keys = list(probability_distribution.keys())\n",
    "    values = list(probability_distribution.values())\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.bar(keys, values)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Items')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def getAttributes(log):\n",
    "    df = pd.DataFrame(log.objects)\n",
    "    typeAttributes = {}\n",
    "    # Iterate over unique types\n",
    "    for type_value in df['ocel:type'].unique():\n",
    "        # Filter DataFrame rows for the current type\n",
    "        type_df = df[df['ocel:type'] == type_value]\n",
    "        \n",
    "        # Get non-empty column names for the current type\n",
    "        non_empty_columns = type_df.dropna(axis=1, how='all').columns.tolist()\n",
    "        \n",
    "        # Store non-empty column names in the dictionary\n",
    "        typeAttributes[type_value] = non_empty_columns\n",
    "    for type_value, column_names in typeAttributes.items():\n",
    "        typeAttributes[type_value] = [col for col in column_names if col not in ['ocel:oid', 'ocel:type']]\n",
    "    return(typeAttributes)\n",
    "\n",
    "def calcORMDs(o2os,type1,type2):\n",
    "    if 'ocel:qualifier' in o2os:\n",
    "        o2os.drop(columns='ocel:qualifier', inplace=True)\n",
    "\n",
    "    obs = extractObjectsDF(adjusted_log)\n",
    "    id_type_dict = {}\n",
    "    for key in obs:\n",
    "        df = obs[key]\n",
    "        new_dict = dict(zip(df['ocel:oid'], df['ocel:type']))\n",
    "        id_type_dict.update(new_dict)\n",
    "\n",
    "    o2os['ocel:type'] = o2os['ocel:oid'].map(id_type_dict)\n",
    "    o2os['ocel:type_2'] = o2os['ocel:oid_2'].map(id_type_dict)\n",
    "    new_order = ['ocel:oid','ocel:type','ocel:oid_2','ocel:type_2']\n",
    "    o2os = o2os[new_order]\n",
    "\n",
    "    df = o2os\n",
    "    # Filter rows based on 'ocel:type' and 'ocel:type_2'\n",
    "    filtered_df = df[df['ocel:type'].isin([type1]) & df['ocel:type_2'].isin([type2])]\n",
    "\n",
    "    \n",
    "    df = filtered_df\n",
    "\n",
    "    # Step 2: Group by 'ocel:oid' and count the number of unique 'ocel:oid_2' values\n",
    "    order_relations_count = filtered_df.groupby('ocel:oid')['ocel:oid_2'].nunique()\n",
    "\n",
    "    # Create a dictionary with 'ocel:oid' of each order as keys and the count as values\n",
    "    order_item_relations = dict(zip(order_relations_count.index, order_relations_count))\n",
    "    values_list = list(order_item_relations.values())\n",
    "    # Count the occurrences of each integer\n",
    "    integer_list = values_list\n",
    "    counts = Counter(integer_list)\n",
    "\n",
    "    # Calculate the total number of elements in the list\n",
    "    total_elements = len(integer_list)\n",
    "\n",
    "    # Create a probability distribution by dividing the count of each integer by the total number of elements\n",
    "    probability_distribution1 = {key: count / total_elements for key, count in counts.items()}\n",
    "\n",
    "    #reverse ORMD:\n",
    "\n",
    "\n",
    "    def swap(row,index):\n",
    "        t2 = row['ocel:type_2']\n",
    "        id2 = row['ocel:oid_2']\n",
    "        df.at[index, 'ocel:type_2'] = row[\"ocel:type\"]\n",
    "        df.at[index, 'ocel:oid_2'] = row[\"ocel:oid\"]\n",
    "        df.at[index, 'ocel:type'] = t2\n",
    "        df.at[index, 'ocel:oid'] = id2\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"ocel:type_2\"] == type2:\n",
    "            swap(row,index)\n",
    "\n",
    "    filtered_df = df\n",
    "\n",
    "    # Step 2: Group by 'ocel:oid' and count the number of unique 'ocel:oid_2' values\n",
    "    order_relations_count = filtered_df.groupby('ocel:oid')['ocel:oid_2'].nunique()\n",
    "\n",
    "    # Create a dictionary with 'ocel:oid' of each order as keys and the count as values\n",
    "    order_item_relations = dict(zip(order_relations_count.index, order_relations_count))\n",
    "    values_list = list(order_item_relations.values())\n",
    "    # Count the occurrences of each integer\n",
    "    integer_list = values_list\n",
    "    counts = Counter(integer_list)\n",
    "\n",
    "    # Calculate the total number of elements in the list\n",
    "    total_elements = len(integer_list)\n",
    "\n",
    "    # Create a probability distribution by dividing the count of each integer by the total number of elements\n",
    "    probability_distribution2 = {key: count / total_elements for key, count in counts.items()}\n",
    "\n",
    "\n",
    "    return(probability_distribution1, probability_distribution2)\n",
    "\n",
    "\n",
    "def support(action,ORMDS,AORMDS):\n",
    "\n",
    "    if action[\"action\"]== \"emit\":\n",
    "        if action[\"direction\"]== 0:\n",
    "            ob1 = action[\"object\"]\n",
    "            X = 0\n",
    "            Y = 0\n",
    "            ob1rels = ob1.relations\n",
    "            for ob in ob1rels:\n",
    "                if ob.type == action[\"type\"]:\n",
    "                    X = X+1\n",
    "            if ((ob1.type,action[\"type\"]) in ORMDS):\n",
    "                ob1perob2 = ORMDS[ob1.type,action[\"type\"]][0]\n",
    "                ob2perob1 = ORMDS[ob1.type,action[\"type\"]][1]\n",
    "            else:\n",
    "                for key, value in AORMDS.items():\n",
    "                    if key[0] == ob1.type and key[1] == action[\"type\"]:\n",
    "                        mia = key[2]\n",
    "                        break\n",
    "                ob1perob2 = AORMDS[ob1.type,action[\"type\"],mia,ob1.attributes[mia]][0]\n",
    "                ob2perob1 = AORMDS[ob1.type,action[\"type\"],mia,ob1.attributes[mia]][1]\n",
    "            \n",
    "\n",
    "            if ob1perob2 == {}:\n",
    "                return 0\n",
    "            if ob2perob1 == {}:\n",
    "                return 0\n",
    "            probXintersect = sum(ob1perob2[x] for x in ob1perob2 if x>=X+1)\n",
    "            probXgeq = sum(ob1perob2[x] for x in ob1perob2 if x>= X)\n",
    "            conditionalProb1 = probXintersect / probXgeq\n",
    "            probYintersect = sum(ob2perob1[x] for x in ob2perob1 if x>=Y+1)\n",
    "            probYgeq = sum(ob2perob1[x] for x in ob2perob1 if x>= Y)\n",
    "            conditionalProb2 = probYintersect / probYgeq\n",
    "\n",
    "\n",
    "            return min(conditionalProb1,conditionalProb2)\n",
    "        if action[\"direction\"]== 1:\n",
    "            ob2 = action[\"object\"]\n",
    "            X = 0\n",
    "            Y = 0\n",
    "            for obj in ob2.reverse_relations:\n",
    "                if obj.type==action[\"type\"]:\n",
    "                    Y=Y+1\n",
    "            if ((action[\"type\"],ob2.type) in ORMDS):\n",
    "                ob1perob2 = ORMDS[action[\"type\"],ob2.type][0]\n",
    "                ob2perob1 = ORMDS[action[\"type\"],ob2.type][1]\n",
    "            else:\n",
    "                for key, value in AORMDS.items():\n",
    "                    if key[0] == action[\"type\"] and key[1] == ob2.type:\n",
    "                        mia = key[2]\n",
    "                        break\n",
    "                ob1perob2 = AORMDS[action[\"type\"],ob2.type,mia,\"PR\"][0]\n",
    "                ob2perob1 = AORMDS[action[\"type\"],ob2.type,mia,\"PR\"][1]\n",
    "            \n",
    "            if ob1perob2 == {}:\n",
    "                return 0\n",
    "            if ob2perob1 == {}:\n",
    "                return 0\n",
    "            probXintersect = sum(ob1perob2[x] for x in ob1perob2 if x>=X+1)\n",
    "            probXgeq = sum(ob1perob2[x] for x in ob1perob2 if x>= X)\n",
    "            conditionalProb1 = probXintersect / probXgeq\n",
    "            probYintersect = sum(ob2perob1[x] for x in ob2perob1 if x>=Y+1)\n",
    "            probYgeq = sum(ob2perob1[x] for x in ob2perob1 if x>= Y)\n",
    "            conditionalProb2 = probYintersect / probYgeq\n",
    "\n",
    "\n",
    "            return min(conditionalProb1,conditionalProb2)\n",
    "\n",
    "    if action[\"action\"]== \"append\":\n",
    "        ob2 = action[\"object2\"]\n",
    "        ob1 = action[\"object1\"]\n",
    "    \n",
    "        if ob2 == ob1:\n",
    "            return 0\n",
    "        X = 0\n",
    "        Y = 0\n",
    "\n",
    "\n",
    "        ob1rels = ob1.relations\n",
    "        for ob in ob1rels:\n",
    "            if ob.type == ob2.type:\n",
    "                X = X+1\n",
    "        ob2rels = ob2.relations\n",
    "        # Zähle wie oft insgesamt auf ob2 gezeigt wird von objekten von typ ob1.type\n",
    "        for obj in ob2.reverse_relations:\n",
    "            if obj.type==ob1.type:\n",
    "                Y=Y+1\n",
    "        #for ob in ob2rels:\n",
    "        #    if ob.type == ob1.type:\n",
    "        #        Y = Y+1\n",
    "        \n",
    "        if ((ob1.type,ob2.type) in ORMDS):\n",
    "            ob1perob2 = ORMDS[ob1.type,ob2.type][0]\n",
    "            ob2perob1 = ORMDS[ob1.type,ob2.type][1]\n",
    "        else:\n",
    "            for key, value in AORMDS.items():\n",
    "                    if key[0] == ob1.type and key[1] == ob2.type:\n",
    "                        mia = key[2]\n",
    "\n",
    "                        ob1perob2 = AORMDS[ob1.type,ob2.type,mia,ob1.attributes[mia]][0]\n",
    "                        ob2perob1 = AORMDS[ob1.type,ob2.type,mia,ob1.attributes[mia]][1]\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "        if ob1perob2 == {}:\n",
    "            return 0\n",
    "        if ob2perob1 == {}:\n",
    "            return 0\n",
    "\n",
    "        probXintersect = sum(ob1perob2[x] for x in ob1perob2 if x>= X and x>=X+1)\n",
    "        probXgeq = sum(ob1perob2[x] for x in ob1perob2 if x>= X)\n",
    "        conditionalProb1 = probXintersect / probXgeq\n",
    "        probYintersect = sum(ob2perob1[x] for x in ob2perob1 if x>= Y and x>=Y+1)\n",
    "        probYgeq = sum(ob2perob1[x] for x in ob2perob1 if x>= Y)\n",
    "        if probYgeq == 0:\n",
    "            return 0\n",
    "        conditionalProb2 = probYintersect / probYgeq\n",
    "        \n",
    "\n",
    "        return min(conditionalProb1,conditionalProb2)\n",
    "        \n",
    "    return 0.5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAORMDs(o2os,objectdf,type1,type2,specialAtt,value):\n",
    "    if 'ocel:qualifier' in o2os:\n",
    "        o2os.drop(columns='ocel:qualifier', inplace=True)\n",
    "    \n",
    "    obs = extractObjectsDF(adjusted_log)\n",
    "    #under the assumption that object of type1 has this particular specialAtt value\n",
    "\n",
    "    \n",
    "    id_type_dict = {}\n",
    "    for key in obs:\n",
    "        df = obs[key]\n",
    "        new_dict = dict(zip(df['ocel:oid'], df['ocel:type']))\n",
    "        id_type_dict.update(new_dict)\n",
    "\n",
    "    o2os['ocel:type'] = o2os['ocel:oid'].map(id_type_dict)\n",
    "    o2os['ocel:type_2'] = o2os['ocel:oid_2'].map(id_type_dict)\n",
    "    new_order = ['ocel:oid','ocel:type','ocel:oid_2','ocel:type_2']\n",
    "    o2os = o2os[new_order]\n",
    "\n",
    "    # Merge o2os with objects to include the specialAtt column based on ocel:oid_1\n",
    "    merged_df = o2os.merge(objectdf[['ocel:oid', specialAtt]], on='ocel:oid', how='left')\n",
    "\n",
    "\n",
    "    \n",
    "    df = merged_df\n",
    "    # Filter rows based on 'ocel:type' and 'ocel:type_2'\n",
    "    filtered_df = df[df['ocel:type'].isin([type1]) & df['ocel:type_2'].isin([type2])&df[specialAtt].isin([value])]\n",
    "\n",
    "    \n",
    "    df = filtered_df\n",
    "\n",
    "\n",
    "    # Step 2: Group by 'ocel:oid' and count the number of unique 'ocel:oid_2' values\n",
    "    order_relations_count = filtered_df.groupby('ocel:oid')['ocel:oid_2'].nunique()\n",
    "\n",
    "    # Create a dictionary with 'ocel:oid' of each order as keys and the count as values\n",
    "    order_item_relations = dict(zip(order_relations_count.index, order_relations_count))\n",
    "    values_list = list(order_item_relations.values())\n",
    "    # Count the occurrences of each integer\n",
    "    integer_list = values_list\n",
    "    counts = Counter(integer_list)\n",
    "\n",
    "    # Calculate the total number of elements in the list\n",
    "    total_elements = len(integer_list)\n",
    "\n",
    "    # Create a probability distribution by dividing the count of each integer by the total number of elements\n",
    "    probability_distribution1 = {key: count / total_elements for key, count in counts.items()}\n",
    "\n",
    "    #reverse ORMD:\n",
    "\n",
    "\n",
    "    def swap(row,index):\n",
    "        t2 = row['ocel:type_2']\n",
    "        id2 = row['ocel:oid_2']\n",
    "        df.at[index, 'ocel:type_2'] = row[\"ocel:type\"]\n",
    "        df.at[index, 'ocel:oid_2'] = row[\"ocel:oid\"]\n",
    "        df.at[index, 'ocel:type'] = t2\n",
    "        df.at[index, 'ocel:oid'] = id2\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"ocel:type_2\"] == type2:\n",
    "            swap(row,index)\n",
    "\n",
    "    filtered_df = df\n",
    "\n",
    "    # Step 2: Group by 'ocel:oid' and count the number of unique 'ocel:oid_2' values\n",
    "    order_relations_count = filtered_df.groupby('ocel:oid')['ocel:oid_2'].nunique()\n",
    "\n",
    "    # Create a dictionary with 'ocel:oid' of each order as keys and the count as values\n",
    "    order_item_relations = dict(zip(order_relations_count.index, order_relations_count))\n",
    "    values_list = list(order_item_relations.values())\n",
    "    # Count the occurrences of each integer\n",
    "    integer_list = values_list\n",
    "    counts = Counter(integer_list)\n",
    "\n",
    "    # Calculate the total number of elements in the list\n",
    "    total_elements = len(integer_list)\n",
    "\n",
    "    # Create a probability distribution by dividing the count of each integer by the total number of elements\n",
    "    probability_distribution2 = {key: count / total_elements for key, count in counts.items()}\n",
    "\n",
    "\n",
    "    return(probability_distribution1, probability_distribution2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log = pm4py.read.read_ocel2_sqlite(\"order-management5.sqlite\")\n",
    "adjusted_log,adjusted_o2os = adjustLog(log)\n",
    "objectdf = adjustLog(log)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcAORMDs(adjusted_o2os,objectdf,\"orders\",\"items\",\"kdtyp\",\"PR\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method takes dataframe of possible attribute values, \n",
    "#type attributes is a map of maps: typeAttributes[type]-> attributes{} ; attributes[att]-> \"independent\" / \"dependent\"\n",
    "\n",
    "def generateIndependendAtts(typeAttributes,objectdf,type,classifier):\n",
    "    attr = {}\n",
    "    for a in typeAttributes[type]:\n",
    "        if typeAttributes[type][a] == \"independent\":\n",
    "            if classifier[a]== \"ca\":\n",
    "                    df = objectdf\n",
    "                    l = df.loc[df['ocel:type'] == type, a].tolist()\n",
    "                    #sampling values\n",
    "                    unique_values, counts = np.unique(l, return_counts=True)\n",
    "                    probabilities = counts / len(l)\n",
    "                    attr[a] = np.random.choice(unique_values,p=probabilities)\n",
    "            if classifier[a]== \"di\":\n",
    "                    df = objectdf\n",
    "                    l = df.loc[df['ocel:type'] == type, a].tolist()\n",
    "                    #sampling values\n",
    "                    unique_values, counts = np.unique(l, return_counts=True)\n",
    "                    probabilities = counts / len(l)\n",
    "                    attr[a] = np.random.choice(unique_values,p=probabilities)\n",
    "            if classifier[a]==\"co\": \n",
    "                    df = objectdf\n",
    "                    l = df.loc[df['ocel:type'] == type, a].tolist()\n",
    "                    #sampling values\n",
    "                    unique_values, counts = np.unique(l, return_counts=True)\n",
    "                    probabilities = counts / len(l)\n",
    "                    attr[a] = np.random.choice(unique_values,p=probabilities)\n",
    "\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDependendAtts(typeAttributes,dependencies,type,independentAtts,distributions,objectdf,classifier):\n",
    "    #get list of dependencies then assign values to dependent attributes accordingly\n",
    "    #dependencies: independentAtt -> dependentAtt[]\n",
    "    #to simplify: every attribute is dependent on a maximum of one independent attribute but multiple attributes can be dependent on this independent attribute\n",
    "    attr = {}\n",
    "    for a in typeAttributes[type]:\n",
    "        if typeAttributes[type][a] == \"dependent\":\n",
    "            #a is the dependent attribute i want to generate\n",
    "            #now i need to find out on which independent attribute it depends\n",
    "            for indAtt in dependencies:\n",
    "                if a in dependencies[indAtt]:\n",
    "                    #indAtt is the independent attribute that 'a' is dependent on\n",
    "                    val = independentAtts[indAtt]\n",
    "                    #check if a distribution of values for my attribute 'a' from datapoints in which my object had the attribute value of indAtt exists\n",
    "                    #if not create one\n",
    "                    if (a,indAtt,val) in distributions:\n",
    "                        dist = distributions[(a,indAtt,val)]\n",
    "                        unique_values = list(dist.keys())\n",
    "                        unique_values = np.array(unique_values)\n",
    "                        probabilities = list(dist.values())\n",
    "                        if classifier[a]==\"ca\":\n",
    "                             attr[a]= random.choices(unique_values,probabilities)[0]\n",
    "                        if classifier[a]==\"co\": \n",
    "                            interpolated_pdf = interp1d(unique_values, probabilities, kind='linear', fill_value=\"extrapolate\")\n",
    "                            \n",
    "                            # Define a fine grid over the range of values\n",
    "                            x_fine = np.linspace(unique_values.min(), unique_values.max(), 1000)\n",
    "                            pdf_fine = interpolated_pdf(x_fine)\n",
    "\n",
    "                            # Integrate the PDF to get the cumulative distribution function (CDF)\n",
    "                            cdf_fine = cumtrapz(pdf_fine, x_fine, initial=0)\n",
    "                            cdf_fine /= cdf_fine[-1]  # Normalize the CDF\n",
    "\n",
    "                            # Function to generate a random value based on the PDF\n",
    "                            def generate_random_value(cdf, x_vals):\n",
    "                                random_uniform = np.random.rand()\n",
    "                                return np.interp(random_uniform, cdf, x_vals)\n",
    "                            # Generate a random value\n",
    "                            random_value = generate_random_value(cdf_fine, x_fine)\n",
    "                            attr[a]= random_value\n",
    "                    else:\n",
    "                        l = objectdf.loc[objectdf[indAtt] == val, a].tolist()\n",
    "                        # Step 1: Count the frequency of each value\n",
    "                        frequency_counts = Counter(l)\n",
    "                        # Step 2: Convert frequencies to probabilities\n",
    "                        total_count = len(l)\n",
    "                        dist = {value: count / total_count for value, count in frequency_counts.items()}\n",
    "                        distributions[(a,indAtt,val)] = dist\n",
    "                        unique_values = list(dist.keys())\n",
    "                        unique_values = np.array(unique_values)\n",
    "                        probabilities = list(dist.values())\n",
    "                        if classifier[a]==\"ca\":\n",
    "                             attr[a]= random.choices(values,probabilities)[0]\n",
    "                        if classifier[a]==\"co\": \n",
    "                            interpolated_pdf = interp1d(unique_values, probabilities, kind='linear', fill_value=\"extrapolate\")\n",
    "                            \n",
    "                            # Define a fine grid over the range of values\n",
    "                            x_fine = np.linspace(unique_values.min(), unique_values.max(), 1000)\n",
    "                            pdf_fine = interpolated_pdf(x_fine)\n",
    "\n",
    "                            # Integrate the PDF to get the cumulative distribution function (CDF)\n",
    "                            cdf_fine = cumtrapz(pdf_fine, x_fine, initial=0)\n",
    "                            cdf_fine /= cdf_fine[-1]  # Normalize the CDF\n",
    "\n",
    "                            # Function to generate a random value based on the PDF\n",
    "                            def generate_random_value(cdf, x_vals):\n",
    "                                random_uniform = np.random.rand()\n",
    "                                return np.interp(random_uniform, cdf, x_vals)\n",
    "                            # Generate a random value\n",
    "                            random_value = generate_random_value(cdf_fine, x_fine)\n",
    "                            attr[a]= random_value\n",
    "                                      \n",
    "    return attr,distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDependendAttsnew(typeAttributes,dependencies,type,independentAtts,distributions,objectdf,parentAtts,o2os,classifier):\n",
    "    #get dependencies then assign values to dependent attributes accordingly\n",
    "    #dependencies: independentAtt,type -> [(dependentAtt,type),...]\n",
    "    #to simplify: every attribute type combination (depAtt,ot) is dependent on a maximum of one independent attribute of a type (indAtt,ot') but multiple attribute type combinations (depAtt1,ot1),...,(depAttn,otn) can be dependent on this independent attribute with corresponding type\n",
    "    attr = {}\n",
    "    for a in typeAttributes[type]:\n",
    "        if typeAttributes[type][a] == \"dependent\":\n",
    "            #a is the dependent attribute I want to generate\n",
    "            #now I need to find out on which independent attribute it depends\n",
    "            for indAtt,otype in dependencies:\n",
    "                if any(tup[0] == a for tup in dependencies[(indAtt, otype)]):\n",
    "                    #indAtt is the independent attribute that 'a' is dependent on\n",
    "                    #otype is the type of object that is involved in the dependency\n",
    "                    if type == otype:\n",
    "                        val = independentAtts[indAtt]\n",
    "                        #check if a distribution of values for my attribute 'a' from datapoints in which my object had the attribute value of indAtt exists\n",
    "                        #if not create one\n",
    "                        if (a,indAtt,val,type) in distributions:\n",
    "                            dist = distributions[(a,indAtt,val,type)]\n",
    "                            unique_values = list(dist.keys())\n",
    "                            unique_values = np.array(unique_values)\n",
    "                            probabilities = list(dist.values())\n",
    "                            attr[a]= random.choices(unique_values,probabilities)[0]\n",
    "                                \n",
    "                        else:\n",
    "                            l = objectdf.loc[(objectdf[indAtt] == val) & (objectdf['ocel:type'] == type), a].tolist()\n",
    "                            # Step 1: Count the frequency of each value\n",
    "                            frequency_counts = Counter(l)\n",
    "                            # Step 2: Convert frequencies to probabilities\n",
    "                            total_count = len(l)\n",
    "                            dist = {value: count / total_count for value, count in frequency_counts.items()}\n",
    "                            distributions[(a,indAtt,val,type)] = dist\n",
    "                            unique_values = list(dist.keys())\n",
    "                            unique_values = np.array(unique_values)\n",
    "                            probabilities = list(dist.values())\n",
    "                            attr[a]= random.choices(unique_values,probabilities)[0]\n",
    "                    else:\n",
    "                        if indAtt in parentAtts:\n",
    "                            val = parentAtts[indAtt]\n",
    "                            if classifier[a]!=\"co\" or classifier[indAtt]!=\"co\": \n",
    "                                if (a,indAtt,val) in distributions:\n",
    "                                    dist = distributions[(a,indAtt,val)]\n",
    "                                    unique_values = list(dist.keys())\n",
    "                                    unique_values = np.array(unique_values)\n",
    "                                    probabilities = list(dist.values())\n",
    "                                    attr[a]= random.choices(unique_values,probabilities)[0]\n",
    "                                else:\n",
    "                                    l = objectdf.loc[(objectdf[indAtt] == val) & (objectdf[\"ocel:type\"] == otype), 'ocel:oid'].tolist() \n",
    "                                    l2 = o2os.loc[(o2os['ocel:oid'] in l) & (o2os['ocel:type']== type),'ocel:oid_2'].tolist()\n",
    "                                    l3 = objectdf.loc[objectdf['ocel:oid'] in l2 , a].tolist()\n",
    "\n",
    "                                    frequency_counts = Counter(l3)\n",
    "                                    total_count = len(l3)\n",
    "                                    dist = {value: count / total_count for value, count in frequency_counts.items()}\n",
    "                                    distributions[(a,indAtt,val,type)] = dist\n",
    "                                    unique_values = list(dist.keys())\n",
    "                                    unique_values = np.array(unique_values)\n",
    "                                    probabilities = list(dist.values())\n",
    "                                    attr[a]= random.choices(unique_values,probabilities)[0]\n",
    "                            else:\n",
    "                                #hier interpolieren \n",
    "                                # Step 1: Filter objectdf to get the mapping from 'ocel:oid' to 'indAtt' for ids in list 'l'\n",
    "                                filtered_objectdf = objectdf[objectdf['ocel:oid'].isin(l)][['ocel:oid', 'indAtt']]\n",
    "\n",
    "                                # Step 2: Merge with o2os to get corresponding 'ocel:oid_2' values\n",
    "                                merged_df = filtered_objectdf.merge(o2os, left_on='ocel:oid', right_on='ocel:oid')[['indAtt', 'ocel:oid_2']]\n",
    "\n",
    "                                # Step 3: Merge again with objectdf to get the 'a' values for the 'ocel:oid_2' objects\n",
    "                                merged_with_a = merged_df.merge(objectdf[['ocel:oid', 'a']], left_on='ocel:oid_2', right_on='ocel:oid', how='left')\n",
    "\n",
    "                                # Step 4: Create a dictionary that maps each 'indAtt' to the corresponding 'a' value of 'ocel:oid_2'\n",
    "                                indAtt_to_a_dict = merged_with_a.set_index('indAtt')['a'].to_dict()\n",
    "                                \n",
    "\n",
    "                                # Step 1: Prepare the data for interpolation\n",
    "                                # Extract the keys and values from the dictionary and sort them by keys for proper interpolation\n",
    "                                indAtt_values = sorted(indAtt_to_a_dict.keys())\n",
    "                                a_values = [indAtt_to_a_dict[key] for key in indAtt_values]\n",
    "\n",
    "                                # Step 2: Create an interpolation function\n",
    "                                # You can choose 'linear', 'quadratic', or 'cubic' based on the smoothness you need\n",
    "                                interpolation_function = interp1d(indAtt_values, a_values, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "                                # Step 3: Define the points you want to sample\n",
    "                                # For example, you might want to sample between the min and max of indAtt_values with a certain resolution\n",
    "                                sample_points = np.linspace(min(indAtt_values), max(indAtt_values), num=50)\n",
    "\n",
    "                                # Step 4: Sample the function and add noise\n",
    "                                # Here, we add Gaussian noise with mean 0 and standard deviation of your choice (e.g., 0.1)\n",
    "                                noise_std_dev = 0.1\n",
    "                                attr[a] = [interpolation_function(x) + random.gauss(0, noise_std_dev) for x in sample_points]\n",
    "\n",
    "                                # Result: sample_points are your x-values, and sampled_values_with_noise are the interpolated y-values with noise\n",
    "\n",
    "                                \n",
    "                                \n",
    "                        else:\n",
    "                            attr[a] = \"MISSING DEPENDENT ATT\"\n",
    "\n",
    "                        \n",
    "\n",
    "                                      \n",
    "    return attr,distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillMissingAttributes(ob1,ob2,dependencies,distributions,objectdf,o2os,classifier):\n",
    "    #find out what attribute values are missing that can be generated with the new related object\n",
    "    #remember that dependencies[a,ot] returns dependent attributes of in independent att a connected to type ot\n",
    "    attr = ob1.attributes\n",
    "    for x in ob1.attributes:\n",
    "        for indAtt in ob2.attributes:\n",
    "            if ob1.attributes[x] == \"MISSING DEPENDENT ATT\" and  any(tup[0] == x for tup in dependencies[(indAtt, ob2.type)]):\n",
    "                #x is the dep att I want to generate\n",
    "                #parentAtt is the ind att it is dependent on\n",
    "                val = ob2.attributes[indAtt]\n",
    "                if (x,indAtt,val) in distributions:\n",
    "                    a=x\n",
    "                    dist = distributions[(a,indAtt,val)]\n",
    "                    unique_values = list(dist.keys())\n",
    "                    unique_values = np.array(unique_values)\n",
    "                    probabilities = list(dist.values())\n",
    "                    attr[a]= random.choices(unique_values,probabilities)[0]\n",
    "                else:\n",
    "                    l = objectdf.loc[objectdf[indAtt] == val, 'ocel:oid'].tolist()\n",
    "                    l2 = o2os.loc[o2os['ocel:oid'] in l ,'ocel:oid_2'].tolist()\n",
    "                    l3 = objectdf.loc[objectdf['ocel:oid'] in l2 , a].tolist()\n",
    "\n",
    "                    frequency_counts = Counter(l3)\n",
    "                    total_count = len(l3)\n",
    "                    dist = {value: count / total_count for value, count in frequency_counts.items()}\n",
    "                    distributions[(a,indAtt,val,type)] = dist\n",
    "                    unique_values = list(dist.keys())\n",
    "                    unique_values = np.array(unique_values)\n",
    "                    probabilities = list(dist.values())\n",
    "                    attr[a]= random.choices(unique_values,probabilities)[0]\n",
    "    return attr\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these just return a dictionary that describes the intended action\n",
    "def emit(object,  type, direction):\n",
    "    return {\"action\": \"emit\", \"object\": object,\"type\": type, \"direction\" : direction}\n",
    "\n",
    "def app(object, object2):\n",
    "    return {\"action\" : \"append\",\"object1\": object, \"object2\":object2}\n",
    "\n",
    "def close(object, type):\n",
    "    return {\"action\":\"close\",\"object\": object,'type' :type}\n",
    "\n",
    "\n",
    "class Object:\n",
    "    def __init__(self,type,attributes, relations, reverse_relations,open_types) -> None:\n",
    "        self.type = type\n",
    "        self.attributes = attributes\n",
    "        self.relations = relations\n",
    "        self.reverse_relations = reverse_relations\n",
    "        self.open_types = open_types\n",
    "\n",
    "def generateGraph(objectTypes, tau, objects, support,log, limit, minObjs,typeAttributes,dependencies,o2os,mias,miaDependencies,classifier,minOrds = 1, maxOrds=2000):\n",
    "    closed = []\n",
    "    R = []\n",
    "    OIxOT = [(item,item2) for item in objects for item2 in objectTypes if item.type == item2]\n",
    "    i=2\n",
    "    ORMDS = {}\n",
    "    AORMDS= {}\n",
    "    objectdf = adjustLog(log)[0]\n",
    "    #typeAttributes = getAttributes(log)\n",
    "    numOfOrds = 1\n",
    "    distributions = {}\n",
    "    \n",
    "    \n",
    "    #ORMD generation\n",
    "    typePairs = [(item,item2) for item in objectTypes for item2 in objectTypes]\n",
    "    for (a,b) in typePairs:\n",
    "        for (specialAtt,type) in mias:\n",
    "            #find out what values specialAtt can have and put them in a list\n",
    "            #miaDependencies describes which multiplicities are dependent on type 1, type 2 and the value of specialAtt from the object of type1\n",
    "            filtered_objectdf = objectdf[objectdf['ocel:type'] == type]\n",
    "            # Get unique values from 'specialAtt' where 'ocel:type' is the specified type\n",
    "            possVal = filtered_objectdf[specialAtt].unique().tolist()\n",
    "            if (a,b,specialAtt) in miaDependencies:\n",
    "                for val in possVal:\n",
    "                    AORMDS[(a,b,specialAtt,val)]=calcAORMDs(o2os,objectdf,a,b,specialAtt,val)\n",
    "            else:\n",
    "                ORMDS[(a,b)] = calcORMDs(o2os,a,b)\n",
    "            \n",
    "        \n",
    "    #special ORMDs for objects that are influenced by attributes\n",
    "\n",
    "    notFullyClosedObjects = []\n",
    "    notFullyClosedObjects= [item for item in objects]\n",
    "    #(len(closed)/len(OIxOT) < tau and\n",
    "\n",
    "    #actual object graph generation\n",
    "    while ((len(objects)<limit) or len(objects)<= minObjs ):\n",
    "        \n",
    "        if numOfOrds>= maxOrds:\n",
    "                print(\"order limit reached\")\n",
    "                return objects,R\n",
    "        \n",
    "            \n",
    "        if notFullyClosedObjects == []:\n",
    "            if numOfOrds>= minOrds:\n",
    "                print(\"minimum order amount reached and all closed\")\n",
    "                return objects,R\n",
    "            else:\n",
    "                attr = {}\n",
    "                attr[\"id\"]= i\n",
    "                independentAtts = generateIndependendAtts(typeAttributes,objectdf,\"orders\",classifier)\n",
    "                dependentAtts,newDistributions = generateDependendAttsnew(typeAttributes,dependencies,\"orders\",independentAtts,distributions,objectdf,[],o2os,classifier)\n",
    "                distributions = newDistributions\n",
    "                attr.update(independentAtts)\n",
    "                attr.update(dependentAtts)\n",
    "                i= i+1\n",
    "                newObject = Object(\"orders\",attr,relations=[],reverse_relations=[],open_types=[x for x in objectTypes])\n",
    "                objects.append(newObject)\n",
    "                notFullyClosedObjects.append(newObject)\n",
    "                numOfOrds = numOfOrds+1\n",
    "                    \n",
    "        o = random.choice(notFullyClosedObjects)\n",
    "        otp = random.choice(o.open_types)\n",
    "        ot = o.type\n",
    "        OIp = [item for item in notFullyClosedObjects if item.type==otp and ((item,ot) and (o,item)) not in R]\n",
    "        A = []\n",
    "        A.append(emit(o,otp,0))\n",
    "        A.append(emit(o,otp,1))\n",
    "        for item in OIp:\n",
    "            A.append(app(o,item))\n",
    "        supportvals = []\n",
    "        for item in A:\n",
    "            supportvals.append(support(item,ORMDS,AORMDS))\n",
    "        wclose = 1-max(supportvals)\n",
    "        Aw = []\n",
    "        for item in A:\n",
    "            Aw.append(support(item,ORMDS,AORMDS))\n",
    "            \n",
    "        aclose = close(o,otp)\n",
    "        A.append(aclose)\n",
    "        Aw.append(wclose)\n",
    "\n",
    "        alpha = random.choices(list(A), weights=list(Aw), k=1)[0]\n",
    "\n",
    "        if alpha == emit(o,otp,0):\n",
    "            attr = {}\n",
    "            attr[\"id\"]= i\n",
    "            \n",
    "            independentAtts = generateIndependendAtts(typeAttributes,objectdf,otp,classifier)\n",
    "            dependentAtts,newDistributions = generateDependendAttsnew(typeAttributes,dependencies,otp,independentAtts,distributions,objectdf,o.attributes,o2os,classifier)\n",
    "            distributions = newDistributions\n",
    "            attr.update(independentAtts)\n",
    "            attr.update(dependentAtts)\n",
    "\n",
    "            i= i+1\n",
    "            newObject = Object(otp,attr,relations=[],reverse_relations=[],open_types=[x for x in objectTypes])\n",
    "            objects.append(newObject)\n",
    "            R.append((o,newObject))\n",
    "            o.relations.append(newObject)\n",
    "            newObject.reverse_relations.append(o)\n",
    "            notFullyClosedObjects.append(newObject)\n",
    "            if otp == \"orders\":\n",
    "                numOfOrds = numOfOrds+1\n",
    "                \n",
    "            for type in objectTypes:\n",
    "                OIxOT.append((newObject,type))\n",
    "                \n",
    "        if alpha == emit(o,otp,1):\n",
    "            attr = {}\n",
    "            attr[\"id\"]= i\n",
    "            \n",
    "            #for att in typeAttributes[otp]:\n",
    "                #df = objectdf\n",
    "               # l = df.loc[df['ocel:type'] == otp, att].tolist()\n",
    "                #sampling values\n",
    "              #  unique_values, counts = np.unique(l, return_counts=True)\n",
    "             #   probabilities = counts / len(l)\n",
    "            #    attr[att] = np.random.choice(unique_values,p=probabilities)\n",
    "            independentAtts = generateIndependendAtts(typeAttributes,objectdf,otp,classifier)\n",
    "            dependentAtts,newDistributions = generateDependendAttsnew(typeAttributes,dependencies,otp,independentAtts,distributions,objectdf,o.attributes,o2os,classifier)\n",
    "            distributions = newDistributions\n",
    "            attr.update(independentAtts)\n",
    "            attr.update(dependentAtts)\n",
    "            \n",
    "            i= i+1\n",
    "            newObject = Object(otp,attr,relations=[],reverse_relations=[],open_types=[x for x in objectTypes])\n",
    "            objects.append(newObject)\n",
    "            R.append((newObject,o))\n",
    "            newObject.relations.append(o)\n",
    "            o.reverse_relations.append(newObject)\n",
    "            notFullyClosedObjects.append(newObject)\n",
    "            if otp == \"orders\":\n",
    "                numOfOrds = numOfOrds+1\n",
    "            for type in objectTypes:\n",
    "                OIxOT.append((newObject,type))\n",
    "        if alpha[\"action\"]== \"append\":\n",
    "             op = alpha[\"object2\"]\n",
    "             o.attributes = fillMissingAttributes(o,op,dependencies,distributions,objectdf,o2os,classifier)\n",
    "             op.attributes = fillMissingAttributes(op,o,dependencies,distributions,objectdf,o2os,classifier)\n",
    "             R.append((o,op))\n",
    "             o.relations.append(op)\n",
    "             op.reverse_relations.append(o)\n",
    "        if alpha == close(o,otp):\n",
    "             closed.append((o,otp))\n",
    "             o.open_types.remove(otp)\n",
    "\n",
    "\n",
    "        if o.open_types==[]:\n",
    "            notFullyClosedObjects.remove(o)\n",
    "        print(\"total objects: \"+str(i))\n",
    "        print(\"number of orders: \"+ str(numOfOrds))\n",
    "        if (len(closed)/len(OIxOT) > tau):\n",
    "            print(\"TAU OUT\")\n",
    "\n",
    "    print(\"LOOP ENDED\")\n",
    "    return (objects,R)\n",
    "\n",
    "def graph(obs, connections):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for (a,b) in obs:\n",
    "        G.add_node(a, label=b)\n",
    "    G.add_edges_from(connections)\n",
    "\n",
    "    pos = nx.spring_layout(G)  \n",
    "    labels = nx.get_node_attributes(G, 'label')\n",
    "    nx.draw(G, pos, with_labels=True, labels=labels, node_size=700, node_color='skyblue', font_size=8, font_color='black', font_weight='bold', arrowsize=10)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pm4py.read.read_ocel2_sqlite(\"order-management5.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLog,newo2os = adjustLog(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a single test graph\n",
    "objectTypes = [\"orders\",\"items\",\"packages\"]\n",
    "att = {}\n",
    "att[\"id\"] = 1\n",
    "seed = Object(\"orders\",att,[],[],[x for x in objectTypes])\n",
    "\n",
    "#if a you want to test with 2 seed objects you need to add seed2 to objects\n",
    "att2 = {}\n",
    "att2[\"id\"] = 18\n",
    "att[\"kdtyp\"] = \"GR\"\n",
    "seed2 = Object(\"orders\",att2,[],[],[x for x in objectTypes])\n",
    "\n",
    "adjusted_log,adjusted_o2os = adjustLog(log)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "objects =[seed]\n",
    "tau = 1\n",
    "packages = {\"weight\":\"independent\", \"volume\":\"dependent\"}\n",
    "items = {\"product\":\"independent\",\"weight\":\"dependent\",\"price\":\"dependent\",\"is_premium\":\"dependent\"}\n",
    "orders =  {\"price\":\"independent\",\"is_premium\":\"independent\",\"kdtyp\":\"independent\"}\n",
    "dependencies = {(\"weight\",\"packages\"):[(\"volume\",\"packages\")] ,(\"product\",\"items\"): [(\"price\",\"items\"),(\"weight\",\"items\"),(\"is_premium\",\"items\")]}\n",
    "#fix dependencies!!!!!! include types\n",
    "typeAttributes = {\"items\":items,\"orders\":orders,\"packages\":packages}\n",
    "#multiplicityInfluencingAttributes\n",
    "mias = [(\"kdtyp\",\"orders\")]\n",
    "\n",
    "miaDependencies = [(\"orders\",\"items\",\"kdtyp\")]\n",
    "\n",
    "\n",
    "attributeClassifier= {\"price\": \"co\",\"weight\": \"co\",\"is_premium\": \"ca\",\"product\":\"ca\",\"volume\":\"co\",\"kdtyp\":\"ca\"}\n",
    "\n",
    "(o,r) = generateGraph(objectTypes=objectTypes, tau=tau, objects=objects,support=support,log=log, limit=12000, minObjs=1,minOrds=200,typeAttributes=typeAttributes,dependencies=dependencies,o2os=adjusted_o2os,mias=mias,miaDependencies=miaDependencies,classifier=attributeClassifier)\n",
    "connections = []\n",
    "for (a,b) in r:\n",
    "    connections.append((int(a.attributes[\"id\"]),int(b.attributes[\"id\"])))\n",
    "\n",
    "obs = []\n",
    "for a in o:\n",
    "    obs.append((int(a.attributes[\"id\"]),a.type))\n",
    "\n",
    "#graph(obs, connections)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in o:\n",
    "    print(str(a.type)+ str(a.attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = 0\n",
    "orders = 0\n",
    "packages = 0\n",
    "for a in o:\n",
    "    if a.type == \"items\":\n",
    "        items = items+1\n",
    "    if a.type == \"orders\":\n",
    "        orders = orders+1\n",
    "    if a.type == \"packages\":\n",
    "        packages = packages+1\n",
    "print(\"items \"+ str(items))\n",
    "print(\"orders \"+ str(orders))\n",
    "print(\"packages \"+ str(packages))\n",
    "print(\"total \" + str(items+orders+packages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs)\n",
    "connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT ORMD TEST FOR ONE GRAPH\n",
    "from collections import defaultdict \n",
    "order_items_counts = defaultdict(int)\n",
    "for ob in o:\n",
    "    if ob.type == \"orders\":\n",
    "        count = 0\n",
    "        for relation in ob.relations:\n",
    "            if relation.type == \"items\":\n",
    "                count = count+1\n",
    "        order_items_counts[count] = order_items_counts[count]+1\n",
    "print(order_items_counts)\n",
    "\n",
    "total = sum(order_items_counts.values())\n",
    "\n",
    "generateditemsPerOrder = {}\n",
    "    \n",
    "# Divide each value by the total to get the probability\n",
    "for key, value in order_items_counts.items():\n",
    "     generateditemsPerOrder[key] = value / total\n",
    "     \n",
    "plot(generateditemsPerOrder,\"simulated items per order\")\n",
    "plot(calcORMDs(o2os=log.o2o,type1=\"orders\",type2=\"items\")[0], \"log based items per order\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemd import emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logItemsPerOrder = calcORMDs(o2os=log.o2o,type1=\"orders\",type2=\"items\")\n",
    "# Example distributions represented as dictionaries\n",
    "distribution1 = logItemsPerOrder[0]\n",
    "distribution2 = generateditemsPerOrder\n",
    "\n",
    "sorted_keys = sorted(distribution1.keys())\n",
    "array1 = np.array([distribution1[key] for key in sorted_keys])\n",
    "sorted_keys = sorted(distribution2.keys())\n",
    "array2 = np.array([distribution2[key] for key in sorted_keys])\n",
    "\n",
    "\n",
    "#pad with zeros to achieve the same histogram length+\n",
    "hist1 = array1\n",
    "hist2 = array2\n",
    "if len(hist1) > len(hist2):\n",
    "    hist2 = np.pad(hist2, (0, len(hist1) - len(hist2)), 'constant')\n",
    "elif len(hist1) < len(hist2):\n",
    "    hist1 = np.pad(hist1, (0, len(hist2) - len(hist1)), 'constant')\n",
    "    \n",
    "array1 = hist1\n",
    "array2 = hist2\n",
    "# Compute the Earth Mover's Distance\n",
    "\n",
    "distance_matrix = np.zeros((len(array1), len(array2)))\n",
    "for i in range(len(array1)):\n",
    "    for j in range(len(array2)):\n",
    "        distance_matrix[i][j] = abs(i - j)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "emd_distance = emd(array1, array2, distance_matrix=distance_matrix)\n",
    "print(\"emd distance: \" +str(emd_distance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace string representations of missing values with np.nan\n",
    "adjusted_log.replace([\"NaN\", \"<NA>\", \"None\"], np.nan, inplace=True)\n",
    "\n",
    "# Convert any actual pd.NA or None to np.nan\n",
    "adjusted_log = adjusted_log.applymap(lambda x: np.nan if pd.isna(x) else x)\n",
    "adjusted_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemPricesGenerated = []\n",
    "for ob in o:\n",
    "    if ob.type == \"items\":\n",
    "        itemPricesGenerated.append(ob.attributes[\"price\"])\n",
    "itemPricesLog = adjusted_log[adjusted_log['ocel:type'] == 'items']['price'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Assume prices1 and prices2 are your two lists of prices\n",
    "plt.hist(itemPricesGenerated, bins=20, alpha=0.5, label='Generated Prices',density=True)\n",
    "plt.hist(itemPricesLog, bins=20, alpha=0.5, label='Log Prices',density=True)\n",
    "\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram Comparison of Prices')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_prices1 = sum(itemPricesGenerated) / len(itemPricesGenerated)\n",
    "average_prices2 = sum(itemPricesLog) / len(itemPricesLog)\n",
    "print(average_prices1)\n",
    "print(average_prices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemWeightsGenerated = []\n",
    "for ob in o:\n",
    "    if ob.type == \"items\":\n",
    "        itemWeightsGenerated.append(ob.attributes[\"weight\"])\n",
    "itemWeightsLog = adjusted_log[adjusted_log['ocel:type'] == 'items']['weight'].tolist()\n",
    "# Assume prices1 and prices2 are your two lists of prices\n",
    "plt.hist(itemWeightsGenerated, bins=20, alpha=0.5, label='Generated Weights',density=True)\n",
    "plt.hist(itemWeightsLog, bins=20, alpha=0.5, label='Log Weights',density=True)\n",
    "\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram Comparison of Weights')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_weights1 = sum(itemWeightsGenerated) / len(itemWeightsGenerated)\n",
    "average_weights2 = sum(itemWeightsLog) / len(itemWeightsLog)\n",
    "print(average_weights1)\n",
    "print(average_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming prices1 and prices2 are your two lists of prices\n",
    "from pyemd import emd\n",
    "# Define the number of bins and the range manually\n",
    "prices1 = itemPricesLog\n",
    "prices2 = itemPricesGenerated\n",
    "\n",
    "num_bins = 10\n",
    "min_edge = min(min(prices1), min(prices2))\n",
    "max_edge = max(max(prices1), max(prices2))\n",
    "bin_edges = np.linspace(min_edge, max_edge, num_bins + 1)\n",
    "\n",
    "# Create histograms with the same bins\n",
    "hist1, _ = np.histogram(prices1, bins=bin_edges, density=True)\n",
    "hist2, _ = np.histogram(prices2, bins=bin_edges, density=True)\n",
    "\n",
    "# Calculate bin centers\n",
    "bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "# Define the distance matrix\n",
    "distance_matrix = np.abs(np.subtract.outer(bin_centers, bin_centers))\n",
    "\n",
    "# Calculate the EMD\n",
    "emd_value = emd(hist1, hist2, distance_matrix)\n",
    "\n",
    "print(f\"EMD between the two distributions: {emd_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyemd import emd\n",
    "\n",
    "weights1 = itemWeightsLog\n",
    "weights2 = itemWeightsGenerated\n",
    "\n",
    "# Assuming weights1 and weights2 are your two lists of weights\n",
    "\n",
    "# Define the number of bins\n",
    "num_bins = 10\n",
    "\n",
    "# Create histograms with the same range\n",
    "hist1, bin_edges = np.histogram(weights1, bins=num_bins, range=(min(min(weights1), min(weights2)), max(max(weights1), max(weights2))), density=True)\n",
    "hist2, _ = np.histogram(weights2, bins=bin_edges, density=True)\n",
    "\n",
    "# Calculate bin centers\n",
    "bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "# Define the distance matrix\n",
    "distance_matrix = np.abs(np.subtract.outer(bin_centers, bin_centers))\n",
    "\n",
    "# Calculate the EMD\n",
    "emd_value = emd(hist1, hist2, distance_matrix)\n",
    "\n",
    "print(f\"EMD between the two distributions: {emd_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EchoPricesLog = adjusted_log[adjusted_log['product'] == 'Echo']['price'].tolist()\n",
    "EchoPricesGenerated = []\n",
    "for ob in o:\n",
    "    if \"product\" in ob.attributes:\n",
    "        if ob.attributes[\"product\"] == \"Echo\":\n",
    "            EchoPricesGenerated.append(ob.attributes[\"price\"])\n",
    "# Assume prices1 and prices2 are your two lists of prices\n",
    "plt.hist(EchoPricesGenerated, bins=20, alpha=0.5, label='Generated Prices',density=True)\n",
    "plt.hist(EchoPricesLog, bins=20, alpha=0.5, label='Log Prices',density=True)\n",
    "\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram Comparison of Prices')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( sum(EchoPricesLog) / len(EchoPricesLog))\n",
    "print(sum(EchoPricesGenerated) / len(EchoPricesGenerated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from pyemd import emd\n",
    "\n",
    "productsGenerated = []\n",
    "for ob in o:\n",
    "    if ob.type == \"items\":\n",
    "        productsGenerated.append(ob.attributes[\"product\"])\n",
    "\n",
    "productsLog = adjusted_log[adjusted_log['ocel:type'] == 'items']['product'].tolist()\n",
    "\n",
    "# Counting frequencies\n",
    "counter1 = collections.Counter(productsGenerated)\n",
    "counter2 = collections.Counter(productsLog)\n",
    "\n",
    "# Creating DataFrames\n",
    "df1 = pd.DataFrame.from_dict(counter1, orient='index', columns=['Generated'])\n",
    "df2 = pd.DataFrame.from_dict(counter2, orient='index', columns=['Log'])\n",
    "\n",
    "# Combine DataFrames and fill missing values with 0\n",
    "df_combined = pd.concat([df1, df2], axis=1).fillna(0).astype(int)\n",
    "\n",
    "# Normalize the frequencies to get probabilities\n",
    "df_combined['Generated'] /= df_combined['Generated'].sum()\n",
    "df_combined['Log'] /= df_combined['Log'].sum()\n",
    "\n",
    "# Sort by the total frequency (optional)\n",
    "df_combined['Total'] = df_combined['Generated'] + df_combined['Log']\n",
    "df_combined = df_combined.sort_values(by='Total', ascending=False).drop(columns='Total')\n",
    "\n",
    "# Convert the probabilities to numpy arrays\n",
    "P = df_combined['Generated'].values\n",
    "Q = df_combined['Log'].values\n",
    "\n",
    "# Create a distance matrix (assuming all products are equally distant)\n",
    "n = len(P)\n",
    "distance_matrix = np.ones((n, n)) - np.eye(n)  # Distance is 1 between different products, 0 for the same product\n",
    "\n",
    "# Compute Earth Mover's Distance (EMD)\n",
    "emd_value = emd(P, Q, distance_matrix)\n",
    "\n",
    "# Print the EMD value\n",
    "print(f\"Earth Mover's Distance: {emd_value}\")\n",
    "\n",
    "# Plotting the normalized distributions with adjustments for clarity\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  # Increased figure size\n",
    "\n",
    "# Plot the data with reduced bar width and transparency\n",
    "df_combined.plot(kind='bar', ax=ax, width=0.7, alpha=0.85)\n",
    "\n",
    "# Labels and Titles\n",
    "plt.title('Normalized Product Frequency Comparison', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Products', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "# Rotate x-axis labels more steeply to prevent overlap\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add horizontal gridlines for easier comparison\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7)\n",
    "\n",
    "# Simplify the annotations to fewer decimal places\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.1%}', (p.get_x() + p.get_width() / 2., height),\n",
    "                    ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# Adjust layout for better fit\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packagesWeightsGenerated = []\n",
    "for ob in o:\n",
    "    if ob.type == \"packages\":\n",
    "        packagesWeightsGenerated.append(ob.attributes[\"weight\"])\n",
    "\n",
    "packagesWeightsLog = adjusted_log[adjusted_log['ocel:type'] == 'packages']['weight'].tolist()\n",
    "\n",
    "# Plot the histograms\n",
    "plt.hist(packagesWeightsGenerated, bins=20, alpha=0.5, label='Generated Weights', density=True)\n",
    "plt.hist(packagesWeightsLog, bins=20, alpha=0.5, label='Log Weights', density=True)\n",
    "\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram Comparison of Weights')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# ---- EMD Calculation ----\n",
    "# Convert weights to numpy arrays\n",
    "weights_generated = np.array(packagesWeightsGenerated)\n",
    "weights_log = np.array(packagesWeightsLog)\n",
    "\n",
    "# Calculate histograms for both generated and log weights (these serve as discrete probability distributions)\n",
    "hist_generated, bin_edges_generated = np.histogram(weights_generated, bins=20, density=True)\n",
    "hist_log, bin_edges_log = np.histogram(weights_log, bins=20, density=True)\n",
    "\n",
    "# Use bin centers as representative values for the weights\n",
    "bin_centers_generated = 0.5 * (bin_edges_generated[1:] + bin_edges_generated[:-1])\n",
    "bin_centers_log = 0.5 * (bin_edges_log[1:] + bin_edges_log[:-1])\n",
    "\n",
    "# Create distance matrix (Euclidean distance between bin centers)\n",
    "n_bins = len(bin_centers_generated)\n",
    "distance_matrix = np.abs(bin_centers_generated.reshape(-1, 1) - bin_centers_generated.reshape(1, -1))\n",
    "\n",
    "# EMD requires the histograms to be normalized, ensuring they sum to 1 (probabilities)\n",
    "hist_generated /= hist_generated.sum()\n",
    "hist_log /= hist_log.sum()\n",
    "\n",
    "# Compute EMD\n",
    "emd_value = emd(hist_generated, hist_log, distance_matrix)\n",
    "\n",
    "# Print the EMD value\n",
    "print(f\"Earth Mover's Distance (EMD) between Generated Weights and Log Weights: {emd_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# Step 1: Filter objects of type 'packages' and extract 'volume' and 'weight' attributes\n",
    "package_data = [(obj.attributes['volume'], obj.attributes['weight']) for obj in o if obj.type == 'packages']\n",
    "\n",
    "# Step 2: Convert to a DataFrame for easier analysis\n",
    "df = pd.DataFrame(package_data, columns=['volume', 'weight'])\n",
    "\n",
    "# Step 3: Check for dependency using correlation\n",
    "# Calculate the Pearson correlation coefficient for linear dependency\n",
    "if not df.empty:\n",
    "    correlation, p_value = pearsonr(df['volume'], df['weight'])\n",
    "    print(f\"Correlation coefficient: {correlation}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "\n",
    "    # Interpretation\n",
    "    if abs(correlation) > 0.7:  # Threshold for strong correlation (adjust as needed)\n",
    "        print(\"There is a strong dependency between volume and weight.\")\n",
    "    else:\n",
    "        print(\"The dependency between volume and weight is weak or nonexistent.\")\n",
    "else:\n",
    "    print(\"No 'packages' objects found in the list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Step 1: Filter and prepare data for plotting\n",
    "package_data = [(obj.attributes['volume'], obj.attributes['weight']) for obj in o if obj.type == 'packages']\n",
    "df = pd.DataFrame(package_data, columns=['volume', 'weight'])\n",
    "\n",
    "# Step 2: Plotting with Seaborn for scatter and optional regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='volume', y='weight', color='blue', label='Generated Data')\n",
    "\n",
    "# Step 3: Add a regression line if you expect a linear dependency\n",
    "slope, intercept, r_value, p_value, std_err = linregress(df['volume'], df['weight'])\n",
    "plt.plot(df['volume'], slope * df['volume'] + intercept, color='red', label=f'Regression line (R={r_value:.2f})')\n",
    "\n",
    "# Step 4: Customize the plot\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Dependency Between Volume and Weight for Packages')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Filter 'adjusted_log' for rows where 'ocel:type' is 'packages' and both 'volume' and 'weight' are non-null\n",
    "filtered_original_df = adjusted_log[(adjusted_log['ocel:type'] == 'packages') & \n",
    "                                    adjusted_log['volume'].notnull() & \n",
    "                                    adjusted_log['weight'].notnull()]\n",
    "\n",
    "# Step 2: Calculate Pearson correlation for both filtered original data and generated data\n",
    "# Original Data (filtered)\n",
    "original_correlation, _ = pearsonr(filtered_original_df['volume'], filtered_original_df['weight'])\n",
    "\n",
    "# Generated Data: Filter for packages only and create DataFrame\n",
    "generated_data = [(obj.attributes['volume'], obj.attributes['weight']) for obj in o if obj.type == 'packages']\n",
    "generated_df = pd.DataFrame(generated_data, columns=['volume', 'weight'])\n",
    "generated_correlation, _ = pearsonr(generated_df['volume'], generated_df['weight'])\n",
    "\n",
    "print(f\"Original Correlation (filtered): {original_correlation:.2f}\")\n",
    "print(f\"Generated Correlation: {generated_correlation:.2f}\")\n",
    "\n",
    "# Step 3: Plot side-by-side scatter plots with regression lines\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# Plot for Original Data (filtered)\n",
    "sns.scatterplot(data=filtered_original_df, x='volume', y='weight', ax=axes[0], color='blue', label='Original Data')\n",
    "slope_orig, intercept_orig, _, _, _ = linregress(filtered_original_df['volume'], filtered_original_df['weight'])\n",
    "axes[0].plot(filtered_original_df['volume'], slope_orig * filtered_original_df['volume'] + intercept_orig, color='red', label=f'Regression (R={original_correlation:.2f})')\n",
    "axes[0].set_title('Original Data (Filtered)')\n",
    "axes[0].set_xlabel('Volume')\n",
    "axes[0].set_ylabel('Weight')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot for Generated Data\n",
    "sns.scatterplot(data=generated_df, x='volume', y='weight', ax=axes[1], color='green', label='Generated Data')\n",
    "slope_gen, intercept_gen, _, _, _ = linregress(generated_df['volume'], generated_df['weight'])\n",
    "axes[1].plot(generated_df['volume'], slope_gen * generated_df['volume'] + intercept_gen, color='red', label=f'Regression (R={generated_correlation:.2f})')\n",
    "axes[1].set_title('Generated Data')\n",
    "axes[1].set_xlabel('Volume')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('Comparison of Volume-Weight Dependency Between Filtered Original and Generated Data')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize weights and volumes from the list\n",
    "weights_list = [obj.attributes['weight'] for obj in o if obj.type == 'packages']\n",
    "volumes_list = [obj.attributes['volume'] for obj in o if obj.type == 'packages']\n",
    "\n",
    "# Compute histograms for the list (normalized to form probability distributions)\n",
    "bins_weights_list = np.linspace(min(weights_list), max(weights_list), 20)\n",
    "hist_weights_list, edges_weights_list = np.histogram(weights_list, bins=bins_weights_list, density=True)\n",
    "\n",
    "bins_volumes_list = np.linspace(min(volumes_list), max(volumes_list), 20)\n",
    "hist_volumes_list, edges_volumes_list = np.histogram(volumes_list, bins=bins_volumes_list, density=True)\n",
    "\n",
    "# Plot normalized distributions from the list\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(edges_weights_list[:-1], hist_weights_list, width=np.diff(edges_weights_list), alpha=0.6, color='blue', label='Weights')\n",
    "plt.title(\"Normalized Weights (List)\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(edges_volumes_list[:-1], hist_volumes_list, width=np.diff(edges_volumes_list), alpha=0.6, color='green', label='Volumes')\n",
    "plt.title(\"Normalized Volumes (List)\")\n",
    "plt.xlabel(\"Volume\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights and volumes from the dataframe\n",
    "weights_df = filtered_original_df['weight']\n",
    "volumes_df = filtered_original_df['volume']\n",
    "\n",
    "# Compute histograms for the dataframe (normalized to form probability distributions)\n",
    "bins_weights_df = np.linspace(weights_df.min(), weights_df.max(), 20)\n",
    "hist_weights_df, edges_weights_df = np.histogram(weights_df, bins=bins_weights_df, density=True)\n",
    "\n",
    "bins_volumes_df = np.linspace(volumes_df.min(), volumes_df.max(), 20)\n",
    "hist_volumes_df, edges_volumes_df = np.histogram(volumes_df, bins=bins_volumes_df, density=True)\n",
    "\n",
    "# Plot normalized distributions from the dataframe\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(edges_weights_df[:-1], hist_weights_df, width=np.diff(edges_weights_df), alpha=0.6, color='orange', label='Weights')\n",
    "plt.title(\"Normalized Weights (DataFrame)\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(edges_volumes_df[:-1], hist_volumes_df, width=np.diff(edges_volumes_df), alpha=0.6, color='red', label='Volumes')\n",
    "plt.title(\"Normalized Volumes (DataFrame)\")\n",
    "plt.xlabel(\"Volume\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "\n",
    "# Ensure the same bins for comparison (combining range from both datasets)\n",
    "combined_bins_weights = np.linspace(\n",
    "    min(min(weights_list), weights_df.min()), \n",
    "    max(max(weights_list), weights_df.max()), \n",
    "    20\n",
    ")\n",
    "combined_bins_volumes = np.linspace(\n",
    "    min(min(volumes_list), volumes_df.min()), \n",
    "    max(max(volumes_list), volumes_df.max()), \n",
    "    20\n",
    ")\n",
    "\n",
    "# Compute normalized histograms with combined bins\n",
    "hist_weights_list_combined, _ = np.histogram(weights_list, bins=combined_bins_weights, density=True)\n",
    "hist_weights_df_combined, _ = np.histogram(weights_df, bins=combined_bins_weights, density=True)\n",
    "\n",
    "hist_volumes_list_combined, _ = np.histogram(volumes_list, bins=combined_bins_volumes, density=True)\n",
    "hist_volumes_df_combined, _ = np.histogram(volumes_df, bins=combined_bins_volumes, density=True)\n",
    "\n",
    "# Compute bin centers for the distance matrix\n",
    "bin_centers_weights = (combined_bins_weights[:-1] + combined_bins_weights[1:]) / 2\n",
    "bin_centers_volumes = (combined_bins_volumes[:-1] + combined_bins_volumes[1:]) / 2\n",
    "\n",
    "# Cost matrices\n",
    "cost_matrix_weights = np.abs(bin_centers_weights[:, None] - bin_centers_weights[None, :])\n",
    "cost_matrix_volumes = np.abs(bin_centers_volumes[:, None] - bin_centers_volumes[None, :])\n",
    "\n",
    "# Compute EMD\n",
    "emd_weights = emd(hist_weights_list_combined, hist_weights_df_combined, cost_matrix_weights)\n",
    "emd_volumes = emd(hist_volumes_list_combined, hist_volumes_df_combined, cost_matrix_volumes)\n",
    "\n",
    "print(f\"EMD for Weights: {emd_weights}\")\n",
    "print(f\"EMD for Volumes: {emd_volumes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderPricesGenerated = []\n",
    "for ob in o:\n",
    "    if ob.type == \"orders\":\n",
    "        if \"price\" in ob.attributes:\n",
    "            orderPricesGenerated.append(ob.attributes[\"price\"])\n",
    "\n",
    "orderPricesLog = adjusted_log[adjusted_log['ocel:type'] == 'orders']['price'].tolist()\n",
    "\n",
    "# Plot the histograms\n",
    "plt.hist(orderPricesGenerated, bins=20, alpha=0.5, label='Generated Prices', density=True)\n",
    "plt.hist(orderPricesLog, bins=20, alpha=0.5, label='Log Prices', density=True)\n",
    "\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram Comparison of Prices')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# ---- EMD Calculation ----\n",
    "# Convert weights to numpy arrays\n",
    "weights_generated = np.array(orderPricesGenerated)\n",
    "weights_log = np.array(orderPricesLog)\n",
    "\n",
    "# Calculate histograms for both generated and log weights (these serve as discrete probability distributions)\n",
    "hist_generated, bin_edges_generated = np.histogram(weights_generated, bins=20, density=True)\n",
    "hist_log, bin_edges_log = np.histogram(weights_log, bins=20, density=True)\n",
    "\n",
    "# Use bin centers as representative values for the weights\n",
    "bin_centers_generated = 0.5 * (bin_edges_generated[1:] + bin_edges_generated[:-1])\n",
    "bin_centers_log = 0.5 * (bin_edges_log[1:] + bin_edges_log[:-1])\n",
    "\n",
    "# Create distance matrix (Euclidean distance between bin centers)\n",
    "n_bins = len(bin_centers_generated)\n",
    "distance_matrix = np.abs(bin_centers_generated.reshape(-1, 1) - bin_centers_generated.reshape(1, -1))\n",
    "\n",
    "# EMD requires the histograms to be normalized, ensuring they sum to 1 (probabilities)\n",
    "hist_generated /= hist_generated.sum()\n",
    "hist_log /= hist_log.sum()\n",
    "\n",
    "# Compute EMD\n",
    "emd_value = emd(hist_generated, hist_log, distance_matrix)\n",
    "\n",
    "# Print the EMD value\n",
    "print(f\"Earth Mover's Distance (EMD) between Generated Prices and Log Prices: {emd_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate dep atts\n",
    "#make distributions of dep atts limited to cases in which dependency present\n",
    "#first dependencies between same object type\n",
    "#dependency (product,items) -> (price,items)\n",
    "ProdPriceLog = []\n",
    "ProdPriceGenerated = []\n",
    "for ob in o:\n",
    "    if ob.type == \"items\" and ob.attributes[\"product\"]==\"iPad Air\":\n",
    "        ProdPriceGenerated.append(ob.attributes[\"price\"])\n",
    "#ProdPriceLog = adjusted_log[adjusted_log['ocel:type'] == 'items' and adjusted_log['product'] == 'iPad Air']['price'].tolist()\n",
    "ProdPriceLog = adjusted_log[(adjusted_log['ocel:type'] == 'items') & (adjusted_log['product'] == 'iPad Air')]['price'].tolist()\n",
    "plt.hist(ProdPriceGenerated, bins=20, alpha=0.5, label='Generated Ipad Price',density=True)\n",
    "plt.hist(ProdPriceLog, bins=20, alpha=0.5, label='Log Ipad Price',density=True)\n",
    "\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram Comparison of Ipad Prices')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate mias\n",
    "#look at items per order depending on value of kdtyp\n",
    "o2os = log.o2o\n",
    "objects = newLog\n",
    "\n",
    "# Step 1: Filter `o2os` to get relationships where orders comprise items\n",
    "order_item_relationships = o2os[o2os['ocel:type'] == 'orders'][['ocel:oid', 'ocel:oid_2']]\n",
    "order_item_relationships = order_item_relationships.rename(columns={'ocel:oid': 'order_oid', 'ocel:oid_2': 'item_oid'})\n",
    "\n",
    "# Step 2: Count the number of items per order\n",
    "item_counts_per_order = order_item_relationships.groupby('order_oid').size().reset_index(name='item_count')\n",
    "\n",
    "# Step 3: Merge item counts with orders in `objects` to add `kdtyp`\n",
    "orders_with_kdtyp = objects[objects['ocel:type'] == 'orders'][['ocel:oid', 'kdtyp']]\n",
    "orders_with_kdtyp = orders_with_kdtyp.rename(columns={'ocel:oid': 'order_oid'})\n",
    "\n",
    "# Merge item counts with orders to add `kdtyp` and item count\n",
    "orders_with_item_counts = orders_with_kdtyp.merge(item_counts_per_order, on='order_oid', how='left')\n",
    "\n",
    "# Step 4: Calculate statistics for item counts by kdtyp\n",
    "kdtyp_analysis = orders_with_item_counts.groupby('kdtyp')['item_count'].agg(\n",
    "    count='count',\n",
    "    mean='mean',\n",
    "    median='median',\n",
    "    min='min',\n",
    "    max='max',\n",
    "    std='std'\n",
    ").fillna(0)  # Fill NaNs with 0 if any `kdtyp` group has no items\n",
    "\n",
    "# Print the result\n",
    "print(\"Dependency between kdtyp and item count:\\n\", kdtyp_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Existing code setup\n",
    "o2os = log.o2o\n",
    "objects = newLog\n",
    "\n",
    "# Step 1: Filter `o2os` to get relationships where orders comprise items\n",
    "order_item_relationships = o2os[o2os['ocel:type'] == 'orders'][['ocel:oid', 'ocel:oid_2']]\n",
    "order_item_relationships = order_item_relationships.rename(columns={'ocel:oid': 'order_oid', 'ocel:oid_2': 'item_oid'})\n",
    "\n",
    "# Step 2: Count the number of items per order\n",
    "item_counts_per_order = order_item_relationships.groupby('order_oid').size().reset_index(name='item_count')\n",
    "\n",
    "# Step 3: Merge item counts with orders in `objects` to add `kdtyp`\n",
    "orders_with_kdtyp = objects[objects['ocel:type'] == 'orders'][['ocel:oid', 'kdtyp']]\n",
    "orders_with_kdtyp = orders_with_kdtyp.rename(columns={'ocel:oid': 'order_oid'})\n",
    "\n",
    "# Merge item counts with orders to add `kdtyp` and item count\n",
    "orders_with_item_counts = orders_with_kdtyp.merge(item_counts_per_order, on='order_oid', how='left')\n",
    "\n",
    "# Step 4: Calculate statistics for item counts by kdtyp\n",
    "kdtyp_analysis = orders_with_item_counts.groupby('kdtyp')['item_count'].agg(\n",
    "    count='count',\n",
    "    mean='mean',\n",
    "    median='median',\n",
    "    min='min',\n",
    "    max='max',\n",
    "    std='std'\n",
    ").fillna(0)  # Fill NaNs with 0 if any `kdtyp` group has no items\n",
    "\n",
    "# Print the result\n",
    "print(\"Dependency between kdtyp and item count:\\n\", kdtyp_analysis)\n",
    "\n",
    "# Step 5: Plot normalized distribution of items per order based on `kdtyp`\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=orders_with_item_counts, x='item_count', hue='kdtyp', kde=True, bins=30, stat=\"density\")\n",
    "plt.title('Normalized Distribution of Items per Order by kdtyp')\n",
    "plt.xlabel('Number of Items per Order')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(title='kdtyp')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, using FacetGrid to show separate normalized distributions for each kdtyp\n",
    "g = sns.FacetGrid(orders_with_item_counts, col='kdtyp', height=4, aspect=1.2)\n",
    "g.map(sns.histplot, 'item_count', kde=True, bins=30, stat=\"density\")\n",
    "g.set_axis_labels('Number of Items per Order', 'Density')\n",
    "g.set_titles(\"kdtyp: {col_name}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `orders_with_item_counts` is prepared as before\n",
    "# Filter data based on `kdtyp` for \"Private Clients\" and \"Corporate Clients\" (adjust labels as needed)\n",
    "private_clients = orders_with_item_counts[orders_with_item_counts['kdtyp'] == 'GR']\n",
    "corporate_clients = orders_with_item_counts[orders_with_item_counts['kdtyp'] == 'PR']\n",
    "\n",
    "\n",
    "\n",
    "# Set up the figure and axis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "# Plot for Private Clients\n",
    "axes[0].hist(private_clients['item_count'], bins=range(0, 17), color='skyblue', density=True, alpha=0.7)\n",
    "axes[0].set_title(\"Items per Order (Private Clients)\")\n",
    "axes[0].set_xlabel(\"Items\")\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "\n",
    "# Plot for Corporate Clients\n",
    "axes[1].hist(corporate_clients['item_count'], bins=range(0, 17), color='lightgreen', density=True, alpha=0.7)\n",
    "axes[1].set_title(\"Items per Order (Corporate Clients)\")\n",
    "axes[1].set_xlabel(\"Items\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "ordersPR = []\n",
    "ordersGR = []\n",
    "for obj in o:\n",
    "    if obj.type == \"items\":\n",
    "        items.append(obj.attributes[\"id\"])\n",
    "for obj in o:\n",
    "    if obj.type == \"orders\" and \"kdtyp\" in obj.attributes:\n",
    "        if obj.attributes[\"kdtyp\"]== \"GR\":\n",
    "            ordersGR.append(obj.attributes[\"id\"])\n",
    "for obj in o:\n",
    "    if obj.type == \"orders\" and \"kdtyp\" in obj.attributes:\n",
    "        if obj.attributes[\"kdtyp\"]== \"PR\":\n",
    "            ordersPR.append(obj.attributes[\"id\"])\n",
    "PR = dict.fromkeys(ordersPR)\n",
    "GR = dict.fromkeys(ordersGR)\n",
    "for x in PR:\n",
    "    PR[x]=0\n",
    "for x in GR:\n",
    "    GR[x]=0\n",
    "for (ob1,ob2) in connections:\n",
    "    if ob1 in ordersPR:\n",
    "        if ob2 in items:\n",
    "            PR[ob1]+=1\n",
    "    if ob1 in ordersGR:\n",
    "        if ob2 in items:\n",
    "            GR[ob1]+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "values1 = GR.values()\n",
    "total1 = sum(values1)\n",
    "count1 = len(values1)\n",
    "# Calculate the average\n",
    "average1 = total1 / count1\n",
    "\n",
    "values2 = PR.values()\n",
    "total2 = sum(values2)\n",
    "count2 = len(values2)\n",
    "# Calculate the average\n",
    "average2 = total2 / count2\n",
    "\n",
    "# Count the occurrences of each value in both lists\n",
    "count1 = Counter(values1)\n",
    "count2 = Counter(values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "ordersPR = []\n",
    "ordersGR = []\n",
    "for obj in o:\n",
    "    if obj.type == \"items\":\n",
    "        items.append(obj.attributes[\"id\"])\n",
    "for obj in o:\n",
    "    if obj.type == \"orders\" and \"kdtyp\" in obj.attributes:\n",
    "        if obj.attributes[\"kdtyp\"]== \"GR\":\n",
    "            ordersGR.append(obj.attributes[\"id\"])\n",
    "for obj in o:\n",
    "    if obj.type == \"orders\" and \"kdtyp\" in obj.attributes:\n",
    "        if obj.attributes[\"kdtyp\"]== \"PR\":\n",
    "            ordersPR.append(obj.attributes[\"id\"])\n",
    "PR = dict.fromkeys(ordersPR)\n",
    "GR = dict.fromkeys(ordersGR)\n",
    "for x in PR:\n",
    "    PR[x]=0\n",
    "for x in GR:\n",
    "    GR[x]=0\n",
    "for (ob1,ob2) in connections:\n",
    "    if ob1 in ordersPR:\n",
    "        if ob2 in items:\n",
    "            PR[ob1]+=1\n",
    "    if ob1 in ordersGR:\n",
    "        if ob2 in items:\n",
    "            GR[ob1]+=1\n",
    "\n",
    "\n",
    "values1 = GR.values()\n",
    "total1 = sum(values1)\n",
    "count1 = len(values1)\n",
    "# Calculate the average\n",
    "average1 = total1 / count1\n",
    "\n",
    "values2 = PR.values()\n",
    "total2 = sum(values2)\n",
    "count2 = len(values2)\n",
    "# Calculate the average\n",
    "average2 = total2 / count2\n",
    "\n",
    "# Count the occurrences of each value in both lists\n",
    "count1 = Counter(values1)\n",
    "count2 = Counter(values2)\n",
    "\n",
    "# Prepare the data for plotting\n",
    "labels1 = list(count1.keys())  # Unique values from the first list\n",
    "counts1 = list(count1.values())  # Counts from the first list\n",
    "\n",
    "labels2 = list(count2.keys())  # Unique values from the second list\n",
    "counts2 = list(count2.values())  # Counts from the second list\n",
    "\n",
    "# Normalize the counts (convert to proportions)\n",
    "total1 = sum(counts1)\n",
    "total2 = sum(counts2)\n",
    "\n",
    "normalized_counts1 = [count / total1 for count in counts1]\n",
    "normalized_counts2 = [count / total2 for count in counts2]\n",
    "\n",
    "# Create a figure with two subplots (side-by-side)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the normalized bar chart for the first list\n",
    "axes[0].bar(labels1, normalized_counts1, color='skyblue')\n",
    "axes[0].set_title('Items per Order (Private Clients)')\n",
    "axes[0].set_xlabel('Items')\n",
    "axes[0].set_ylabel('Probability')\n",
    "\n",
    "# Plot the normalized bar chart for the second list\n",
    "axes[1].bar(labels2, normalized_counts2, color='lightgreen')\n",
    "axes[1].set_title('Items per Order (Corporate Clients)')\n",
    "axes[1].set_xlabel('Items')\n",
    "axes[1].set_ylabel('Probability')\n",
    "\n",
    "# Show the charts\n",
    "plt.tight_layout()  # Adjust layout to avoid overlapping\n",
    "plt.show()\n",
    "\n",
    "print(\"GR average items per order:\" + str(average1))\n",
    "print(\"PR average items per order:\" + str(average2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyemd import emd\n",
    "\n",
    "# Define the bins (common for both log and simulation data)\n",
    "bins = np.arange(0, 17)  # Bins from the histogram\n",
    "\n",
    "# Log-generated distributions (normalized histograms)\n",
    "private_hist, _ = np.histogram(private_clients['item_count'], bins=bins, density=True)\n",
    "corporate_hist, _ = np.histogram(corporate_clients['item_count'], bins=bins, density=True)\n",
    "\n",
    "# Simulation-generated distributions\n",
    "# Align and normalize distributions for Private Clients\n",
    "sim_private_probs = np.zeros(len(bins) - 1)  # Initialize with zeros\n",
    "for i, label in enumerate(labels1):\n",
    "    sim_private_probs[bins[:-1] == label] = normalized_counts1[i]\n",
    "\n",
    "# Align and normalize distributions for Corporate Clients\n",
    "sim_corporate_probs = np.zeros(len(bins) - 1)  # Initialize with zeros\n",
    "for i, label in enumerate(labels2):\n",
    "    sim_corporate_probs[bins[:-1] == label] = normalized_counts2[i]\n",
    "\n",
    "# Compute the distance matrix (using bin centers)\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])  # Bin centers\n",
    "distance_matrix = np.abs(bin_centers[:, None] - bin_centers[None, :])  # Pairwise distances\n",
    "\n",
    "# Calculate EMD for Private Clients\n",
    "emd_private = emd(private_hist, sim_private_probs, distance_matrix)\n",
    "\n",
    "# Calculate EMD for Corporate Clients\n",
    "emd_corporate = emd(corporate_hist, sim_corporate_probs, distance_matrix)\n",
    "\n",
    "# Print results\n",
    "print(f\"EMD between Private Clients (Log vs Simulation): {emd_private}\")\n",
    "print(f\"EMD between Corporate Clients (Log vs Simulation): {emd_corporate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the bins (common for both log and simulation data)\n",
    "bins = np.arange(0, 17)  # Bins from the histogram\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])  # Bin centers\n",
    "\n",
    "# Normalize histograms (log-generated distributions)\n",
    "private_hist, _ = np.histogram(private_clients['item_count'], bins=bins, density=True)\n",
    "corporate_hist, _ = np.histogram(corporate_clients['item_count'], bins=bins, density=True)\n",
    "\n",
    "# Simulation-generated distributions (aligned bins)\n",
    "sim_private_probs = np.zeros(len(bins) - 1)  # Initialize with zeros\n",
    "for i, label in enumerate(labels1):\n",
    "    sim_private_probs[bins[:-1] == label] = normalized_counts1[i]\n",
    "\n",
    "sim_corporate_probs = np.zeros(len(bins) - 1)  # Initialize with zeros\n",
    "for i, label in enumerate(labels2):\n",
    "    sim_corporate_probs[bins[:-1] == label] = normalized_counts2[i]\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot for Private Clients\n",
    "axes[0].bar(bin_centers, private_hist, width=0.5, color='skyblue', alpha=0.7, label='Log-Generated')\n",
    "axes[0].bar(bin_centers, sim_private_probs, width=0.3, color='orange', alpha=0.7, label='Simulation')\n",
    "axes[0].set_title(\"Items per Order (Private Clients)\")\n",
    "axes[0].set_xlabel(\"Items\")\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot for Corporate Clients\n",
    "axes[1].bar(bin_centers, corporate_hist, width=0.5, color='lightgreen', alpha=0.7, label='Log-Generated')\n",
    "axes[1].bar(bin_centers, sim_corporate_probs, width=0.3, color='purple', alpha=0.7, label='Simulation')\n",
    "axes[1].set_title(\"Items per Order (Corporate Clients)\")\n",
    "axes[1].set_xlabel(\"Items\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
